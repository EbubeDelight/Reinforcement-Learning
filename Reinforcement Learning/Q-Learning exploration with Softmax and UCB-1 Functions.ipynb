{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa67b3a2-dfc2-473b-9f5b-f2f9e6e901ca",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981a982-83dc-4253-a50d-c39da2c2601a",
   "metadata": {},
   "source": [
    "## TD control: Q-Learning (off-policy)\n",
    "* Problems to consider: Gridworld_environment\n",
    "* Q-Learning: $Q(S,A) := Q(S,A) + \\alpha \\Big [ R(S') + \\gamma \\max_a Q(S′, a) − Q(S, A) \\Big ]$\n",
    "* Remarks: \n",
    "    * A is chosen from S using policy derived from Q (e.g., Softmax and ucb-1)\n",
    "    * In Q-Learning, the \"best\" action a is taken in $\\max_a Q(S′, a)$ irrespective of the control policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc418b6-6c2c-421f-8cab-4c24c0fb02cd",
   "metadata": {},
   "source": [
    "# Simulated environment: The Grid-World environment\n",
    "Consider the grid world environment in which an agent begins at an initial location (0,0) and tries to get to the target location (3,3), one step at a time. In the 4x4 grid world, the agent randomly selects any action \n",
    "* Action Space: The agent can take discrete actions: move up, down, left, and right.\n",
    "* Rewards:\n",
    "    * The agent gets a reward of -1 as it moves from one state to another after taking an action. This negative  reward (i.e. penalty) enforces the agent to complete the journey and reach the terminal state in minimum number of steps.\n",
    "    * If the agent wants to run out of the grid, it gets a penalty of -5. Example, the agent receives a penalty of -5, if it goes LEFTward from any of these states (0, 0), (0, 1), (0, 2), (0, 3).\n",
    "    * The agent also gets a reward of 1, whenever it moves to the terminal state (3,3) from any of these two states (2, 3), (3, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a58b8-d67a-4d67-aed0-4302f3b21400",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "Python Code snippet from Professor Stefan Fauser's jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9763cd99-1950-437b-a89e-fb9d47a5d8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652d0aa-a858-4aac-a612-ddfa12ec507c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### environment Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9be521c9-bbe4-4985-a241-720c79dcf1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gridworld_sim_env(S, A, optional_args=None, verbose=False):\n",
    "    nrow, ncol = 4, 4  # 4x4 grid world environment\n",
    "    \n",
    "    if optional_args is not None:\n",
    "        penalty_outside_grid = optional_args.get(\"penalty_outside_grid\", -5)\n",
    "        penalty_movt = optional_args.get(\"penalty_movt\", -1)\n",
    "        goal_reward= optional_args.get(\"goal_reward\", 1)\n",
    "    else:\n",
    "        penalty_outside_grid = -5\n",
    "        penalty_movt = -1\n",
    "        goal_reward= 1\n",
    "    \n",
    "    if verbose: \n",
    "        print(\"S =\", S, \"A =\", A)\n",
    "    \n",
    "    curr_row= S // ncol\n",
    "    curr_col = S % ncol\n",
    "    col_mx = curr_col - 1\n",
    "    row_mx = curr_row - 1\n",
    "    col_mn = curr_col + 1\n",
    "    row_mn = curr_row + 1\n",
    "    \n",
    "    if A == 0:  # Go left\n",
    "        next_col = max(0, col_mx)\n",
    "        next_row = curr_row\n",
    "    elif A == 1:  # Go up\n",
    "        next_row = max(0, row_mx)\n",
    "        next_col = curr_col\n",
    "    elif A == 2:  # Go right\n",
    "        next_col = min(ncol - 1, col_mn)\n",
    "        next_row = curr_row\n",
    "    elif A == 3:  # Go down\n",
    "        next_row = min(nrow - 1, row_mn)\n",
    "        next_col = curr_col\n",
    "    \n",
    "    if (next_row, next_col) == (3, 3):  # The Terminal state\n",
    "        R = goal_reward\n",
    "        S_prime = None  # Terminal state\n",
    "    elif next_row < 0 or next_col < 0 or next_row >= nrow or next_col >= ncol:\n",
    "        R = penalty_outside_grid  # reward for moving outside the grid\n",
    "        S_prime = S  # remain in same state\n",
    "    else:\n",
    "        R = penalty_movt\n",
    "        A_dd = next_row * ncol\n",
    "        S_prime = A_dd + next_col\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"new r =\", next_row, \"new c =\", next_col, \"S_prime =\", S_prime, \"R =\", R)\n",
    "    \n",
    "    return R, S_prime\n",
    "\n",
    "def gridworld_sim_env_print_Q(Q):    \n",
    "    for r in range(4):\n",
    "        print(\"-------------------------\\n\", end=\"\")\n",
    "        for A in range(4):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(4):\n",
    "                S = r * 4 + c\n",
    "                if (r, c) == (3, 3):\n",
    "                    print(\"G|\", end=\"\")  # goal\n",
    "                elif A == 0:\n",
    "                    print(\"L: {:06.2f}|\".format(np.round(Q[S, A], 2)), end=\"\")\n",
    "                elif A == 1:\n",
    "                    print(\"U: {:06.2f}|\".format(np.round(Q[S, A], 2)), end=\"\")\n",
    "                elif A == 2:\n",
    "                    print(\"R: {:06.2f}|\".format(np.round(Q[S, A], 2)), end=\"\")\n",
    "                elif A == 3:\n",
    "                    print(\"D: {:06.2f}|\".format(np.round(Q[S, A], 2)), end=\"\")\n",
    "            print(\"\\n\", end=\"\")\n",
    "    print(\"-------------------------\\n\", end=\"\")\n",
    "\n",
    "\n",
    "def gridworld_sim_env_policy(Q):    \n",
    "    for r in range(4):\n",
    "        print(\"-------------------------\\n|\", end=\"\")\n",
    "        for c in range(4):\n",
    "            S = r * 4 + c\n",
    "            if (r, c) == (3, 3):\n",
    "                print(\"G|\", end=\"\")  # goal\n",
    "            else:\n",
    "                rd_q = np.round(Q[S,], 2)\n",
    "                A = np.argmax(rd_q)\n",
    "                if A == 0:\n",
    "                    print(\"L|\", end=\"\")\n",
    "                elif A == 1:\n",
    "                    print(\"U|\", end=\"\")\n",
    "                elif A == 2:\n",
    "                    print(\"R|\", end=\"\")\n",
    "                elif A == 3:\n",
    "                    print(\"D|\", end=\"\")\n",
    "        print(\"\\n\", end=\"\")\n",
    "    print(\"-------------------------\\n\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1b50c1a-f304-4244-9d52-07a36e95c351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S = 0 A = 2\n",
      "new r = 0 new c = 1 S_prime = 1 R = -1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridworld_sim_env(0, 2, verbose=True) # go to the right (2) from start (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f66223f3-6ec3-4af3-9f10-f31416891aa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning with UCB-1 Exploration\n",
      "Policy after 1000 episodes:\n",
      "-------------------------\n",
      "|R|R|R|D|\n",
      "-------------------------\n",
      "|R|R|D|D|\n",
      "-------------------------\n",
      "|R|R|R|D|\n",
      "-------------------------\n",
      "|R|R|R|G|\n",
      "-------------------------\n",
      "Q values:\n",
      "-------------------------\n",
      "|L: -04.01|L: -03.26|L: -02.88|L: -02.13|\n",
      "|U: -04.01|U: -03.37|U: -02.65|U: -01.85|\n",
      "|R: -03.50|R: -02.78|R: -01.98|R: -01.85|\n",
      "|D: -03.60|D: -02.90|D: -01.98|D: -01.09|\n",
      "-------------------------\n",
      "|L: -03.37|L: -03.07|L: -02.18|L: -01.48|\n",
      "|U: -03.50|U: -02.83|U: -02.26|U: -01.30|\n",
      "|R: -02.98|R: -02.12|R: -01.58|R: -01.43|\n",
      "|D: -02.98|D: -02.28|D: -01.09|D: -00.10|\n",
      "-------------------------\n",
      "|L: -03.02|L: -02.40|L: -01.79|L: -00.80|\n",
      "|U: -02.93|U: -02.29|U: -01.78|U: -00.72|\n",
      "|R: -02.34|R: -01.30|R: -00.10|R: -00.50|\n",
      "|D: -02.34|D: -01.88|D: -00.78|D: 001.00|\n",
      "-------------------------\n",
      "|L: -02.26|L: -01.77|L: -01.19|G|\n",
      "|U: -02.26|U: -02.05|U: -00.72|G|\n",
      "|R: -01.38|R: -00.18|R: 001.00|G|\n",
      "|D: -02.26|D: -01.85|D: -00.98|G|\n",
      "-------------------------\n",
      "Policy after 10000 episodes:\n",
      "-------------------------\n",
      "|R|R|R|D|\n",
      "-------------------------\n",
      "|R|R|D|D|\n",
      "-------------------------\n",
      "|R|R|R|D|\n",
      "-------------------------\n",
      "|R|R|R|G|\n",
      "-------------------------\n",
      "Q values:\n",
      "-------------------------\n",
      "|L: -04.01|L: -03.26|L: -02.88|L: -02.13|\n",
      "|U: -04.01|U: -03.37|U: -02.65|U: -01.85|\n",
      "|R: -03.50|R: -02.78|R: -01.98|R: -01.85|\n",
      "|D: -03.60|D: -02.90|D: -01.98|D: -01.09|\n",
      "-------------------------\n",
      "|L: -03.37|L: -03.07|L: -02.18|L: -01.48|\n",
      "|U: -03.50|U: -02.83|U: -02.26|U: -01.30|\n",
      "|R: -02.98|R: -02.12|R: -01.58|R: -01.43|\n",
      "|D: -02.98|D: -02.28|D: -01.09|D: -00.10|\n",
      "-------------------------\n",
      "|L: -03.02|L: -02.40|L: -01.79|L: -00.80|\n",
      "|U: -02.93|U: -02.29|U: -01.78|U: -00.72|\n",
      "|R: -02.34|R: -01.30|R: -00.10|R: -00.50|\n",
      "|D: -02.34|D: -01.88|D: -00.78|D: 001.00|\n",
      "-------------------------\n",
      "|L: -02.26|L: -01.77|L: -01.19|G|\n",
      "|U: -02.26|U: -02.05|U: -00.72|G|\n",
      "|R: -01.38|R: -00.18|R: 001.00|G|\n",
      "|D: -02.26|D: -01.85|D: -00.98|G|\n",
      "-------------------------\n",
      "Policy after 20000 episodes:\n",
      "-------------------------\n",
      "|R|R|R|D|\n",
      "-------------------------\n",
      "|R|R|D|D|\n",
      "-------------------------\n",
      "|R|R|R|D|\n",
      "-------------------------\n",
      "|R|R|R|G|\n",
      "-------------------------\n",
      "Q values:\n",
      "-------------------------\n",
      "|L: -04.01|L: -03.26|L: -02.88|L: -02.13|\n",
      "|U: -04.01|U: -03.37|U: -02.65|U: -01.85|\n",
      "|R: -03.50|R: -02.78|R: -01.98|R: -01.85|\n",
      "|D: -03.60|D: -02.90|D: -01.98|D: -01.09|\n",
      "-------------------------\n",
      "|L: -03.37|L: -03.07|L: -02.18|L: -01.48|\n",
      "|U: -03.50|U: -02.83|U: -02.26|U: -01.30|\n",
      "|R: -02.98|R: -02.12|R: -01.58|R: -01.43|\n",
      "|D: -02.98|D: -02.28|D: -01.09|D: -00.10|\n",
      "-------------------------\n",
      "|L: -03.02|L: -02.40|L: -01.79|L: -00.80|\n",
      "|U: -02.93|U: -02.29|U: -01.78|U: -00.72|\n",
      "|R: -02.34|R: -01.30|R: -00.10|R: -00.50|\n",
      "|D: -02.34|D: -01.88|D: -00.78|D: 001.00|\n",
      "-------------------------\n",
      "|L: -02.26|L: -01.77|L: -01.19|G|\n",
      "|U: -02.26|U: -02.05|U: -00.72|G|\n",
      "|R: -01.38|R: -00.18|R: 001.00|G|\n",
      "|D: -02.26|D: -01.85|D: -00.98|G|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "#ucb-1 Function\n",
    "def ucb_1(Q, N, t):\n",
    "    slog = np.log(t + 1)\n",
    "    tlog = N + 1e-6\n",
    "    exploration_bonus = np.sqrt( slog / tlog)\n",
    "    Q_exp = Q + exploration_bonus\n",
    "    return Q_exp\n",
    "\n",
    "def ucb_1_take_action(Q, N, S, t):\n",
    "    slog = np.log(t + 1)\n",
    "    tlog =  N[S] + 1e-6\n",
    "    exploration_bonus = np.sqrt(slog / tlog)\n",
    "    action_values = Q[S, :] + exploration_bonus\n",
    "    return np.argmax(action_values)\n",
    "\n",
    "def update_Q_ucb_1(Q, N, S, A, R, S_prime, alpha, gamma, t):\n",
    "    if S_prime is None:  # Q[S_prime, ...] means terminal state\n",
    "        Q[S, A] = Q[S, A] + alpha * (R - Q[S, A])\n",
    "    else:  # Q-learning; search best action by using UCB-1\n",
    "        best_action = np.argmax(ucb_1(Q[S_prime, :], N[S_prime, :], t))\n",
    "        Q[S, A] = Q[S, A] + alpha * (R + gamma * Q[S_prime, best_action] - Q[S, A])\n",
    "    return Q\n",
    "\n",
    "\n",
    "\n",
    "def gridworld_sim_env_loop_ucb_1(episodes, alpha, gamma, optional_args=None):\n",
    "    maximum_states = 16  # since grid world is a 4x4 grid\n",
    "    maximum_actions = 4  # Actions: 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "    Q = np.zeros((maximum_states, maximum_actions))\n",
    "    N = np.zeros((maximum_states, maximum_actions))\n",
    "    for episode in range(1, episodes + 1):\n",
    "        S = 0  # start state = 0\n",
    "        t = 1  # time step\n",
    "        while S is not None:\n",
    "            Q, N, S = agent_sim_env_interaction_ucb_1(gridworld_sim_env, optional_args, Q, N, S, None, t, alpha=alpha, gamma=gamma)\n",
    "            t += 1\n",
    "    return Q\n",
    "\n",
    "def agent_sim_env_interaction_ucb_1(env_func, optional_args, Q, N, S, A, t, alpha=0.5, gamma=0.9):\n",
    "    if A is None:\n",
    "        A = ucb_1_take_action(Q, N, S, t)\n",
    "    R, S_prime = env_func(S, A, optional_args=optional_args)\n",
    "    Q = update_Q_ucb_1(Q, N, S, A, R, S_prime, alpha, gamma, t)\n",
    "    N[S, A] += 1  # Update the action count\n",
    "    return Q, N, S_prime\n",
    "\n",
    "\n",
    "\n",
    "# Print the Q values for UCB-1 function in a grid world environment\n",
    "print(\"Q-Learning with UCB-1 Exploration\")\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "for T in [1000, 10000, 20000]:\n",
    "    np.random.seed(777)\n",
    "    Q_ucb_1 = gridworld_sim_env_loop_ucb_1(T, alpha, gamma, optional_args=None)  # Update optional_args\n",
    "    print(\"Policy after\", T, \"episodes:\")\n",
    "    gridworld_sim_env_policy(Q_ucb_1)\n",
    "    print(\"Q values:\")\n",
    "    gridworld_sim_env_print_Q(Q_ucb_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d280068e-c164-4d7c-bdc9-2f26ecc7202e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning with Softmax Exploration\n",
      "Policy after 1000 episodes:\n",
      "-------------------------\n",
      "|R|D|D|D|\n",
      "-------------------------\n",
      "|R|D|D|D|\n",
      "-------------------------\n",
      "|R|R|D|D|\n",
      "-------------------------\n",
      "|R|R|R|G|\n",
      "-------------------------\n",
      "Q values:\n",
      "-------------------------\n",
      "|L: -02.00|L: -02.00|L: -01.99|L: -01.96|\n",
      "|U: -02.00|U: -01.99|U: -01.97|U: -01.91|\n",
      "|R: -01.99|R: -01.97|R: -01.90|R: -01.93|\n",
      "|D: -01.99|D: -01.95|D: -01.86|D: -01.66|\n",
      "-------------------------\n",
      "|L: -01.99|L: -01.99|L: -01.97|L: -01.88|\n",
      "|U: -02.00|U: -01.99|U: -01.96|U: -01.90|\n",
      "|R: -01.96|R: -01.87|R: -01.63|R: -01.62|\n",
      "|D: -01.98|D: -01.86|D: -01.55|D: -00.69|\n",
      "-------------------------\n",
      "|L: -01.97|L: -01.97|L: -01.87|L: -01.50|\n",
      "|U: -01.99|U: -01.96|U: -01.89|U: -01.61|\n",
      "|R: -01.87|R: -01.49|R: -00.90|R: -00.75|\n",
      "|D: -01.91|D: -01.63|D: -00.67|D: 001.00|\n",
      "-------------------------\n",
      "|L: -01.94|L: -01.93|L: -01.58|G|\n",
      "|U: -01.96|U: -01.88|U: -01.52|G|\n",
      "|R: -01.67|R: -00.75|R: 001.00|G|\n",
      "|D: -01.93|D: -01.77|D: -00.77|G|\n",
      "-------------------------\n",
      "Policy after 10000 episodes:\n",
      "-------------------------\n",
      "|R|R|D|D|\n",
      "-------------------------\n",
      "|R|R|D|D|\n",
      "-------------------------\n",
      "|R|R|D|D|\n",
      "-------------------------\n",
      "|R|R|R|G|\n",
      "-------------------------\n",
      "Q values:\n",
      "-------------------------\n",
      "|L: -02.00|L: -02.00|L: -01.99|L: -01.97|\n",
      "|U: -02.00|U: -01.98|U: -01.96|U: -01.92|\n",
      "|R: -01.99|R: -01.95|R: -01.92|R: -01.94|\n",
      "|D: -01.99|D: -01.96|D: -01.83|D: -01.76|\n",
      "-------------------------\n",
      "|L: -01.99|L: -01.99|L: -01.96|L: -01.87|\n",
      "|U: -02.00|U: -01.99|U: -01.97|U: -01.91|\n",
      "|R: -01.96|R: -01.88|R: -01.63|R: -01.76|\n",
      "|D: -01.98|D: -01.89|D: -01.45|D: -00.96|\n",
      "-------------------------\n",
      "|L: -01.98|L: -01.98|L: -01.87|L: -01.46|\n",
      "|U: -01.99|U: -01.97|U: -01.81|U: -01.62|\n",
      "|R: -01.89|R: -01.55|R: -00.82|R: -00.77|\n",
      "|D: -01.94|D: -01.71|D: -00.64|D: 001.00|\n",
      "-------------------------\n",
      "|L: -01.95|L: -01.96|L: -01.68|G|\n",
      "|U: -01.98|U: -01.85|U: -01.64|G|\n",
      "|R: -01.69|R: -00.74|R: 001.00|G|\n",
      "|D: -01.96|D: -01.67|D: -00.69|G|\n",
      "-------------------------\n",
      "Policy after 20000 episodes:\n",
      "-------------------------\n",
      "|R|R|D|D|\n",
      "-------------------------\n",
      "|D|D|D|D|\n",
      "-------------------------\n",
      "|R|R|R|D|\n",
      "-------------------------\n",
      "|R|R|R|G|\n",
      "-------------------------\n",
      "Q values:\n",
      "-------------------------\n",
      "|L: -02.00|L: -02.00|L: -01.99|L: -01.96|\n",
      "|U: -02.00|U: -01.99|U: -01.98|U: -01.95|\n",
      "|R: -01.99|R: -01.98|R: -01.95|R: -01.93|\n",
      "|D: -01.99|D: -01.98|D: -01.89|D: -01.51|\n",
      "-------------------------\n",
      "|L: -01.99|L: -01.99|L: -01.96|L: -01.88|\n",
      "|U: -02.00|U: -01.99|U: -01.98|U: -01.93|\n",
      "|R: -01.98|R: -01.89|R: -01.62|R: -01.60|\n",
      "|D: -01.96|D: -01.85|D: -01.58|D: -00.55|\n",
      "-------------------------\n",
      "|L: -01.97|L: -01.96|L: -01.89|L: -01.49|\n",
      "|U: -01.99|U: -01.96|U: -01.89|U: -01.75|\n",
      "|R: -01.87|R: -01.58|R: -00.94|R: -00.78|\n",
      "|D: -01.91|D: -01.61|D: -01.02|D: 001.00|\n",
      "-------------------------\n",
      "|L: -01.93|L: -01.94|L: -01.67|G|\n",
      "|U: -01.97|U: -01.85|U: -01.60|G|\n",
      "|R: -01.74|R: -00.67|R: 001.00|G|\n",
      "|D: -01.94|D: -01.56|D: -00.65|G|\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "#Softmax Function\n",
    "def softmax(x, temp=1.0):\n",
    "    e_x = np.exp((x - np.max(x)) / temp)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "def update_Q_softmax(Q, S, A, R, S_prime, alpha, gamma, temp):\n",
    "    if S_prime is None:  # Q[S_prime, ...] means terminal state\n",
    "        Q[S, A] = Q[S, A] + alpha * (R - Q[S, A])\n",
    "    else:  #  # Q-learning; search best action by using softmax\n",
    "        prob = softmax(Q[S_prime, :], temp)\n",
    "        best_action = np.random.choice(len(prob), p=prob)\n",
    "        Q[S, A] = Q[S, A] + alpha * (R + gamma * Q[S_prime, best_action] - Q[S, A])\n",
    "    return Q\n",
    "\n",
    "\n",
    "def softmax_take_action(Q, S, temp=1.0):\n",
    "    prob = softmax(Q[S, :], temp)\n",
    "    action = np.random.choice(len(prob), p=prob)\n",
    "    return action\n",
    "\n",
    "def agent_sim_env_interaction_softmax(env_func, optional_args, Q, S, A, temp=1.0, alpha=0.5, gamma=0.9):\n",
    "    if A is None:\n",
    "        A = softmax_take_action(Q, S, temp)\n",
    "    R, S_prime = env_func(S, A, optional_args=optional_args)\n",
    "    Q = update_Q_softmax(Q, S, A, R, S_prime, alpha, gamma, temp)\n",
    "    return Q, S_prime\n",
    "\n",
    "def gridworld_sim_env_loop_softmax(episodes, alpha, gamma, temp=1.0, optional_args=None):\n",
    "    maximum_states = 16  # since grid world is a 4x4 grid\n",
    "    maximum_actions = 4  # Actions: 0 - Left, 1 - Down, 2 - Right, 3 - Up\n",
    "    Q = np.zeros((maximum_states, maximum_actions))\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        S = 0  # start state = 0\n",
    "        A = None\n",
    "        while S is not None:\n",
    "            Q, S = agent_sim_env_interaction_softmax(gridworld_sim_env, optional_args, Q, S, A, temp=temp, alpha=alpha, gamma=gamma)\n",
    "    return Q\n",
    "\n",
    "\n",
    "# Print the Q values for softmax function in the grid world environment\n",
    "print(\"Q-Learning with Softmax Exploration\")\n",
    "episodes_list = [1000, 10000, 20000]\n",
    "alpha = 0.1\n",
    "gamma = 0.5\n",
    "\n",
    "for T in episodes_list:\n",
    "    np.random.seed(777)\n",
    "    Q = gridworld_sim_env_loop_softmax(T, alpha, gamma, temp=1.0, optional_args=None)\n",
    "    print(f\"Policy after {T} episodes:\")\n",
    "    gridworld_sim_env_policy(Q)\n",
    "    print(\"Q values:\")\n",
    "    gridworld_sim_env_print_Q(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ae659b7-fdee-4835-9f10-2c894da0fa20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9925590113476528 -1.9963418282157885\n",
      "-1.9969489923084016 -1.9984744961542007\n"
     ]
    }
   ],
   "source": [
    "# left hand and right hand values should be equal\n",
    "print(Q[0, 2], -1 + gamma * Q[0, 3]) # go right from upper left state\n",
    "print(Q[0, 1], -1 + gamma * Q[0, 1]) # go up from start state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ddac1d-4cfd-4b67-953c-8490289d3378",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c5539ae-64b3-4430-9096-27339366133d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters (UCB-1 with Gaussian Process):\n",
      "[0.22213099 0.48141397]\n",
      "Best Hyperparameters (Softmax with Gaussian Process):\n",
      "[0.22213099 0.48141397 0.21786919]\n"
     ]
    }
   ],
   "source": [
    "# function for Gaussian Process\n",
    "def kernel(X1, X2, l=1e-2, sigma_f=1.0):\n",
    "    distance_matrix = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n",
    "    sig_w = np.exp(-0.5 / l**2 * distance_matrix)\n",
    "    return sigma_f**2 * sig_w # Calculate the Gaussian kernel matrix using RBF\n",
    "\n",
    "# fitting a Gaussian Process model to the provided data\n",
    "def fit_guassian_process(X, y, l=1e-2, sigma_f=1.0):\n",
    "    K_nel = kernel(X, X, l)\n",
    "    m_eye = 1e-9 * np.eye(len(X))\n",
    "    K = K_nel + m_eye\n",
    "    L = np.linalg.cholesky(K)\n",
    "    alpha = np.linalg.solve(L.T, np.linalg.solve(L, y))\n",
    "    \n",
    "    # use new input data (X_in) to make predictions\n",
    "    def predict(X_in): \n",
    "       k_in = kernel(X, X_in, l)\n",
    "       Lk = np.linalg.solve(L,k_in)\n",
    "       predicted_mean = np.dot(Lk.T, alpha)\n",
    "\n",
    "       k_in2 = kernel(X_in, X_in, l)\n",
    "       cov = k_in2 - np.dot(Lk.T, Lk)\n",
    "       return predicted_mean, cov\n",
    "\n",
    "    return predict\n",
    "#Objective function\n",
    "def objective_func(acquisition_func, *hyperparameters, **kwargs):\n",
    "    if acquisition_func == 'ucb-1':\n",
    "        alpha, gamma, *rest = hyperparameters\n",
    "        neg_ht_ucb_1 = ((alpha - 0.5)**2 + (gamma - 0.5)**2) * -1\n",
    "        return neg_ht_ucb_1  # Objective for UCB-1\n",
    "    elif acquisition_func == 'softmax':\n",
    "        alpha, gamma, temp = hyperparameters\n",
    "        neg_ht_softmax = (((alpha - 0.5)**2 + (gamma - 0.5)**2) + (temp - 1.0)**2) * -1\n",
    "        return neg_ht_softmax  # Objective for Softmax\n",
    "    else:\n",
    "        raise ValueError(\"Invalid acquisition function. Select 'ucb-1' or 'softmax'.\")\n",
    "\n",
    "\n",
    "def softmax_acquisition_func(mean, std, temp=1.0):\n",
    "    mx_mn = mean - np.max(mean)\n",
    "    std_tmp = temp * std + 1e-9\n",
    "    exponentiated_values = np.exp(mx_mn / std_tmp)\n",
    "    prob = exponentiated_values / np.sum(exponentiated_values)\n",
    "    return prob\n",
    "\n",
    "def bayesian_optimization(n_iterations, bound, acquisition_func='ucb-1', **kwargs):\n",
    "    np.random.seed(777)\n",
    "    X = np.random.rand(n_iterations, len(bound))\n",
    "    y = np.zeros(n_iterations)\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        x_in_hyperparameter_space = bound[:, 0] + X[i] * (bound[:, 1] - bound[:, 0])\n",
    "\n",
    "        y[i] = objective_func(acquisition_func, *x_in_hyperparameter_space, **kwargs)\n",
    "\n",
    "        gp_predict = fit_guassian_process(X[:i + 1], y[:i + 1])\n",
    "        X_in = np.random.rand(1, len(bound))  # Generate a random point for prediction\n",
    "        mean, cov = gp_predict(X_in)\n",
    "\n",
    "        if acquisition_func == 'ucb-1':\n",
    "            acquisition_values = ucb_1_acquisition_func(mean, np.sqrt(np.diag(cov)))\n",
    "        elif acquisition_func == 'softmax':\n",
    "            acquisition_values = softmax_acquisition_func(mean, np.sqrt(np.diag(cov)), **kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid acquisition function. Choose 'ucb-1' or 'softmax'.\")\n",
    "\n",
    "        # Select next point with maximum acquisition value\n",
    "        next_point = X[np.argmax(acquisition_values)]\n",
    "\n",
    "        x_in_hyperparameter_space = bound[:, 0] + next_point * (bound[:, 1] - bound[:, 0])\n",
    "        y_next = objective_func(acquisition_func, *x_in_hyperparameter_space, **kwargs)\n",
    "\n",
    "        X[i] = next_point\n",
    "        y[i] = y_next\n",
    "\n",
    "    best_index = np.argmax(y)\n",
    "    best_hyperparameters = bound[:, 0] + X[best_index] * (bound[:, 1] - bound[:, 0])\n",
    "\n",
    "    return best_hyperparameters\n",
    "\n",
    "def ucb_1_acquisition_func(mean, std, exploration_tradeoff=0.1):\n",
    "    mn_expo = mean + exploration_tradeoff * std\n",
    "    return mn_expo\n",
    "\n",
    "# bound for hyperparameters\n",
    "bound = np.array([[0.1, 0.9], [0.3, 0.9], [0.1, 2.0]])\n",
    "\n",
    "# Bayesian Optimization for UCB-1 using Gaussian Process\n",
    "best_hyperparameters_ucb_1 = bayesian_optimization(n_iterations=100, bound=bound[:3], acquisition_func='ucb-1')\n",
    "\n",
    "# Bayesian Optimization with Softmax using Gaussian Process\n",
    "best_hyperparameters_softmax = bayesian_optimization(n_iterations=100, bound=bound, acquisition_func='softmax', temp=1.0)\n",
    "\n",
    "print(\"Best Hyperparameters (UCB-1 with Gaussian Process):\")\n",
    "print(best_hyperparameters_ucb_1[:-1])\n",
    "\n",
    "print(\"Best Hyperparameters (Softmax with Gaussian Process):\")\n",
    "print(best_hyperparameters_softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3c780-8b16-404d-ad36-600e07047bd4",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "\n",
    "* Both exploration strategies show a convergence to a similar optimal policy, highlighting right and down actions to arrive at the goal.\n",
    "* However, the Q-values for UCB-1 exploration depicts a more stable convergence, while that of softmax exploration shows a gradual convergence with some fluctuations .\n",
    "* The observed policies and Q-values for the two strategies indicate a comparable performance in learning the optimal policy for the simulated grid world environment.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "* In this context, there is a notable similarity in the performance of softmax and UCB-1 exploration functions.\n",
    "* Choosing between the two may depend on specific features of the environment and computational considerations, as both functions demonstrate effective learning in this context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (default)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
